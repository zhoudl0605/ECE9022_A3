{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medmnist import BloodMNIST\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import VGG16_Weights\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# Select device to accelerate the computation\n",
    "#check if CUDA is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#check if MPS is available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else device)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "train_dataset = BloodMNIST(\n",
    "    root='./datasets/', split='train', download=True, transform=transform)\n",
    "val_dataset = BloodMNIST(root='./datasets/', split='val',\n",
    "                         download=True, transform=transform)\n",
    "test_dataset = BloodMNIST(root='./datasets/', split='test',\n",
    "                          download=True, transform=transform)\n",
    "\n",
    "\n",
    "# Data Loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lable: 7\n",
      "Image size: torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# get sample data\n",
    "sample, label = train_dataset[0]\n",
    "print(f\"Lable: {label.item()}\")\n",
    "print(f\"Image size: {sample.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vgg16_model():\n",
    "    # Load pre-trained VGG16 model\n",
    "    model = models.vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "    # Remove the existing classification head by replacing it\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(25088, 4096),  # First fully connected layer (unchanged)\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 256),  # Replace second FC layer with smaller one\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(256, 8),  # Binary classification (fracture vs. non-fracture)\n",
    "        nn.Softmax(dim=1)  # Softmax for classification\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_rgb_image_np(gray_image_np):\n",
    "    return np.dstack([gray_image_np, gray_image_np, gray_image_np])\n",
    "\n",
    "\n",
    "def to_rgb_image_nps(gray_image_nps):\n",
    "    for i in range(len(gray_image_nps)):\n",
    "        gray_image_nps[i] = to_rgb_image_np(gray_image_nps[i])\n",
    "    return gray_image_nps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer=None, class_criterion=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(dataloader, desc=\"Training batch\", leave=False):\n",
    "        images = images.to(device)\n",
    "        labels = labels.view(-1).long().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = class_criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def validate(model, dataloader, class_criterion=None):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Validation batch\", leave=False):\n",
    "            images = images.to(device)\n",
    "            labels = labels.view(-1).long().to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = class_criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def test(model, dataloader, class_criterion=None):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "    show_sample = True\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Testing batch\", leave=False):\n",
    "            images = images.to(device)\n",
    "            labels = labels.view(-1).long().to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = class_criterion(outputs, labels) if class_criterion else 0.0\n",
    "\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "            if show_sample:\n",
    "                count = 0\n",
    "                # print the sample of misclassified images\n",
    "                for i in range(len(labels)):\n",
    "                    if labels[i] != outputs.argmax(dim=1)[i]:\n",
    "                        print(\n",
    "                            f\"True: {labels[i].item()}, Predicted: {outputs.argmax(dim=1)[i].item()}\")\n",
    "                        plt.imshow(images[i].cpu().permute(1, 2, 0))\n",
    "                        plt.show()\n",
    "                        count += 1\n",
    "                        if count == 3:\n",
    "                            show_sample = False\n",
    "                            break\n",
    "\n",
    "    # 计算指标\n",
    "    acc = accuracy_score(y_true, y_pred) * 100\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='weighted', zero_division=0)\n",
    "    avg_loss = running_loss / total_samples if total_samples > 0 else 0.0\n",
    "\n",
    "    print(f\"Accuracy: {acc:.2f}%\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 结果字典\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'loss': avg_loss\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "265b60571cde48549de7a46a6d780c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93210fb1d3bc4fe5a1a46cb83e07cefc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batch:   0%|          | 0/374 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# use tqdm for progress bar\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs), desc=\u001b[33m\"\u001b[39m\u001b[33mEpochs\u001b[39m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# Train for one epoch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     train_loss_epoch = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     train_loss.append(train_loss_epoch)\n\u001b[32m     22\u001b[39m     \u001b[38;5;66;03m# Validate on validation set\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, dataloader, optimizer, class_criterion)\u001b[39m\n\u001b[32m      2\u001b[39m model.train()\n\u001b[32m      3\u001b[39m running_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTraining batch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/assign3/lib/python3.12/site-packages/tqdm/notebook.py:250\u001b[39m, in \u001b[36mtqdm_notebook.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    249\u001b[39m     it = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__iter__\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/assign3/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/assign3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/assign3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/assign3/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/assign3/lib/python3.12/site-packages/medmnist/dataset.py:144\u001b[39m, in \u001b[36mMedMNIST2D.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    141\u001b[39m     img = img.convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    147\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/assign3/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/assign3/lib/python3.12/site-packages/torchvision/transforms/transforms.py:137\u001b[39m, in \u001b[36mToTensor.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/assign3/lib/python3.12/site-packages/torchvision/transforms/functional.py:174\u001b[39m, in \u001b[36mto_tensor\u001b[39m\u001b[34m(pic)\u001b[39m\n\u001b[32m    172\u001b[39m img = img.view(pic.size[\u001b[32m1\u001b[39m], pic.size[\u001b[32m0\u001b[39m], F_pil.get_image_num_channels(pic))\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m img = \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch.ByteTensor):\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img.to(dtype=default_float_dtype).div(\u001b[32m255\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train VGG16\n",
    "# Use Cross-Entropy Loss and Adam optimizer.\n",
    "model = get_vgg16_model()\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 2\n",
    "early_stop = 5\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "min_val_loss = np.inf\n",
    "early_stop_counter = 0\n",
    "best_model = None\n",
    "\n",
    "# use tqdm for progress bar\n",
    "for epoch in tqdm(range(epochs), desc=\"Epochs\", leave=True):\n",
    "    # Train for one epoch\n",
    "    train_loss_epoch = train(\n",
    "        model, train_loader, optimizer, criterion)\n",
    "    train_loss.append(train_loss_epoch)\n",
    "\n",
    "    # Validate on validation set\n",
    "    val_loss_epoch = validate(model, val_loader, criterion)\n",
    "    val_loss.append(val_loss_epoch)\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_loss_epoch < min_val_loss:\n",
    "        min_val_loss = val_loss_epoch\n",
    "        early_stop_counter = 0\n",
    "        best_model = model.state_dict()\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "\n",
    "    if early_stop_counter >= early_stop:\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "# Plot the results\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae6cefe536a4a89b7117fb88eca7a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing batch:   0%|          | 0/107 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 19.47%\n",
      "Precision: 0.04\n",
      "Recall: 0.19\n",
      "F1 Score: 0.06\n",
      "Average Loss: 2.0793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 19.467991815258696,\n",
       " 'precision': 0.037900270531897956,\n",
       " 'recall': 0.19467991815258695,\n",
       " 'f1_score': 0.06344840983098748,\n",
       " 'loss': 2.0793288317872434}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model\n",
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "\n",
    "# calculate accuracy, precision, recall, and F1 score\n",
    "test(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Erosion(image):\n",
    "    kernel = np.ones((3, 3), np.uint8)  # 3x3 结构元素\n",
    "    return cv2.erode(image, kernel, iterations=1)\n",
    "\n",
    "def Skeletonization(image):\n",
    "    skel = np.zeros(image.shape, np.uint8)  # 骨架结果初始化\n",
    "    ret, image = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)  # 二值化\n",
    "    element = cv2.getStructuringElement(cv2.MORPH_CROSS, (3, 3))  # 交叉形结构元素\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        eroded = cv2.erode(image, element)  # 腐蚀\n",
    "        temp = cv2.dilate(eroded, element)  # 先腐蚀再膨胀 (开运算)\n",
    "        temp = cv2.subtract(image, temp)  # 提取去掉的部分\n",
    "        skel = cv2.bitwise_or(skel, temp)  # 累加骨架\n",
    "        image = eroded.copy()\n",
    "\n",
    "        if cv2.countNonZero(image) == 0:  # 如果图像完全腐蚀完毕，停止\n",
    "            done = True\n",
    "\n",
    "    return skel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_erosion = True\n",
    "class ErosionTransform:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, image):\n",
    "        print(f\"image type: {image.mode}\")\n",
    "        if first_erosion:\n",
    "            plt.imshow(image)\n",
    "            plt.show()\n",
    "        gray_image = image.convert(\"L\")\n",
    "\n",
    "        gray_image_np = np.array(gray_image)\n",
    "        gray_image_np = cv2.erodes(\n",
    "            gray_image_np, None, iterations=1)\n",
    "\n",
    "        rgb_image_np = np.dstack(\n",
    "            [gray_image_np, gray_image_np, gray_image_np])\n",
    "\n",
    "        rgb_image = Image.fromarray(rgb_image_np)\n",
    "        if first_erosion:\n",
    "            plt.imshow(rgb_image)\n",
    "            plt.show()\n",
    "            first_erosion = False\n",
    "\n",
    "        return rgb_image\n",
    "\n",
    "\n",
    "first_skeletonization = True\n",
    "class SkeletonizationTransform:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, image):\n",
    "        if first_skeletonization:\n",
    "            plt.imshow(image)\n",
    "            plt.show()\n",
    "        \n",
    "        gray_image = image.convert(\"L\")\n",
    "\n",
    "        gray_image_np = np.array(gray_image)\n",
    "        gray_image_np = Skeletonization(gray_image_np)\n",
    "\n",
    "        rgb_image_np = np.dstack(\n",
    "            [gray_image_np, gray_image_np, gray_image_np])\n",
    "\n",
    "        rgb_image = Image.fromarray(rgb_image_np)\n",
    "        if first_skeletonization:\n",
    "            plt.imshow(rgb_image)\n",
    "            plt.show()\n",
    "            first_skeletonization = False\n",
    "\n",
    "        return rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trasnform = transforms.Compose([\n",
    "    ErosionTransform(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "train_dataset = BloodMNIST(\n",
    "    root='./datasets/', split='train', download=True, transform=transform)\n",
    "val_dataset = BloodMNIST(root='./datasets/', split='val',\n",
    "                         download=True, transform=transform)\n",
    "test_dataset = BloodMNIST(root='./datasets/', split='test',\n",
    "                          download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train VGG16\n",
    "# Use Cross-Entropy Loss and Adam optimizer.\n",
    "model = get_vgg16_model()\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 2\n",
    "early_stop = 5\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "min_val_loss = np.inf\n",
    "early_stop_counter = 0\n",
    "best_model = None\n",
    "\n",
    "# use tqdm for progress bar\n",
    "for epoch in tqdm(range(epochs), desc=\"Epochs\", leave=True):\n",
    "    # Train for one epoch\n",
    "    train_loss_epoch = train(\n",
    "        model, train_loader, optimizer, criterion)\n",
    "    train_loss.append(train_loss_epoch)\n",
    "\n",
    "    # Validate on validation set\n",
    "    val_loss_epoch = validate(model, val_loader, criterion)\n",
    "    val_loss.append(val_loss_epoch)\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_loss_epoch < min_val_loss:\n",
    "        min_val_loss = val_loss_epoch\n",
    "        early_stop_counter = 0\n",
    "        best_model = model.state_dict()\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "\n",
    "    if early_stop_counter >= early_stop:\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "# Plot the results\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "\n",
    "# calculate accuracy, precision, recall, and F1 score\n",
    "test(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trasnform = transforms.Compose([\n",
    "    SkeletonizationTransform(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "train_dataset = BloodMNIST(\n",
    "    root='./datasets/', split='train', download=True, transform=transform)\n",
    "val_dataset = BloodMNIST(root='./datasets/', split='val',\n",
    "                         download=True, transform=transform)\n",
    "test_dataset = BloodMNIST(root='./datasets/', split='test',\n",
    "                          download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train VGG16\n",
    "# Use Cross-Entropy Loss and Adam optimizer.\n",
    "model = get_vgg16_model()\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 2\n",
    "early_stop = 5\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "min_val_loss = np.inf\n",
    "early_stop_counter = 0\n",
    "best_model = None\n",
    "\n",
    "# use tqdm for progress bar\n",
    "for epoch in tqdm(range(epochs), desc=\"Epochs\", leave=True):\n",
    "    # Train for one epoch\n",
    "    train_loss_epoch = train(\n",
    "        model, train_loader, optimizer, criterion)\n",
    "    train_loss.append(train_loss_epoch)\n",
    "\n",
    "    # Validate on validation set\n",
    "    val_loss_epoch = validate(model, val_loader, criterion)\n",
    "    val_loss.append(val_loss_epoch)\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_loss_epoch < min_val_loss:\n",
    "        min_val_loss = val_loss_epoch\n",
    "        early_stop_counter = 0\n",
    "        best_model = model.state_dict()\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "\n",
    "    if early_stop_counter >= early_stop:\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "# Plot the results\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "\n",
    "# calculate accuracy, precision, recall, and F1 score\n",
    "test(model, test_loader, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assign3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
